# Atoma Paged Attention

![Atoma Logo](https://github.com/atoma-network/atoma-paged-attention/blob/atoma-image/assets/atoma-symbol.jpg)

A collection of Large Language Models (LLMs) optimized for KV cache memory management, through Paged Attention see [here](https://arxiv.org/pdf/2309.06180). In particular, Paged Attention allows for optimized inference serving, which is crucial for Atoma nodes.
